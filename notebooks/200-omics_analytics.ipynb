{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying annotations and variants with Omics Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to help you get started importing variant and annotation data into a variety of Omics Analytics stores. We will cover:\n",
    "\n",
    "1. VCF and gVCF inputs\n",
    "2. Annotation data in VCF, TSV, and GFF formats\n",
    "3. How to write queries on these data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "### Python requirements\n",
    "* Python >= 3.8\n",
    "* Packages:\n",
    "  * boto3 >= 1.29.22\n",
    "  * botocore >= 1.29.22\n",
    "  * [AWS Wrangler](https://pypi.org/project/awswrangler/) >= 2.17.0\n",
    "\n",
    "### AWS requirements\n",
    "\n",
    "#### AWS CLI\n",
    "You will need the AWS CLI installed and configured in your environment. Supported AWS CLI versions are:\n",
    "\n",
    "* AWS CLI v2 >= 2.9.3 (Recommended)\n",
    "* AWS CLI v1 >= 1.27.19\n",
    "\n",
    "#### AWS Region\n",
    "Amazon Omics is currently available in Oregon (us-west-2), N. Virginia (us-east-1), Dublin (eu-west-1), London (eu-west-2), Frankfurt (eu-central-1), and Singapore (ap-southeast-1).\n",
    "\n",
    "As written, this notebook works best in the `us-west-2` AWS Region. The datasets it uses are all openly available via the [Registry of Open Data on AWS](https://registry.opendata.aws) and are stored in buckets in `us-west-2`.\n",
    "\n",
    "Amazon Omics only allows importing data within the same region. In addition, source buckets created after Sep 23, 2020 must named such that they are compatible with Virtual-hosted-style requests. For this notebook to work in other regions, you will need to have the following data available in an S3 bucket in your selected region:\n",
    "\n",
    "* s3://1000genomes-dragen/data/precisionFDA/hg38-graph-based/\n",
    "* s3://aws-genomics-datasets/omics-e2e/clinvar.vcf.gz\n",
    "\n",
    "#### Output buckets\n",
    "You will need a bucket in the same region you are running this tutorial in to store query outputs.\n",
    "\n",
    "#### Amazon Omics references\n",
    "You need to have the following Amazon Omics resources available:\n",
    "\n",
    "* Reference store\n",
    "* A reference in the Reference store named \"GRCh38\"\n",
    "\n",
    "For more information on how to create these, see the Amazon Omics Storage tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "import boto3\n",
    "import botocore.exceptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a service IAM role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, we will use the following policy and trust policy to demo the capabilities of Amazon Omics, you are free to customize permissions as required for your use case after going though this tutorial.\n",
    "\n",
    "**NOTE:**\n",
    "In this case we've defined rather permissive permissions (i.e. \"\\*\" Resources). In particular, we are allowing read/write access to all S3 buckets available to the account for this tutorial. In a real world setting you will want to scope this down to only the minimally needed actions on necessary resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a timestamp\n",
    "dt_fmt = '%Y%m%dT%H%M%S'\n",
    "ts = datetime.now().strftime(dt_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"omics:*\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ram:AcceptResourceShareInvitation\",\n",
    "        \"ram:GetResourceShareInvitations\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"s3:GetBucketLocation\",\n",
    "        \"s3:PutObject\",\n",
    "        \"s3:GetObject\",\n",
    "        \"s3:ListBucket\",\n",
    "        \"s3:AbortMultipartUpload\",\n",
    "        \"s3:ListMultipartUploadParts\",\n",
    "        \"s3:GetObjectAcl\",\n",
    "        \"s3:PutObjectAcl\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "demo_trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"omics.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to proceed we need to create a couple of resources the first is the role that you will be passing into Amazon Omics. If the role doesn't exist, we will need to create it and attach the policy and trust policy defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this as the base name for our role and policy\n",
    "omics_iam_name = f'OmicsTutorialRole-{ts}'\n",
    "\n",
    "# Create the iam client\n",
    "iam = boto3.resource('iam')\n",
    "\n",
    "# Check if the role already exist if not create it\n",
    "try:\n",
    "    role = iam.Role(omics_iam_name)\n",
    "    role.load()\n",
    "    \n",
    "except botocore.exceptions.ClientError as ex:\n",
    "    if ex.response[\"Error\"][\"Code\"] == \"NoSuchEntity\":\n",
    "        #Create the role with the corresponding trust policy\n",
    "        role = iam.create_role(\n",
    "            RoleName=omics_iam_name, \n",
    "            AssumeRolePolicyDocument=json.dumps(demo_trust_policy))\n",
    "        \n",
    "        #Create policy\n",
    "        policy = iam.create_policy(\n",
    "            PolicyName='{}-policy'.format(omics_iam_name), \n",
    "            Description=\"Policy for Amazon Omics demo\",\n",
    "            PolicyDocument=json.dumps(demo_policy))\n",
    "        \n",
    "        #Attach the policy to the role\n",
    "        policy.attach_role(RoleName=omics_iam_name)\n",
    "    else:\n",
    "        print('Somthing went wrong, please retry and check your account settings and permissions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the role exist, lets create a helper function to help us retrieve the role arn which we will need to pass into the service API calls. The role arn will grant Amazon Omics the permissions it needs to access the resources it needs in your aws account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_role_arn(role_name):\n",
    "    try:\n",
    "        iam = boto3.resource('iam')\n",
    "        role = iam.Role(role_name)\n",
    "        role.load()  # calls GetRole to load attributes\n",
    "    except ClientError:\n",
    "        print(\"Couldn't get role named %s.\"%role_name)\n",
    "        raise\n",
    "    else:\n",
    "        return role.arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Amazon Omics client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = boto3.client('omics', region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant Store\n",
    "\n",
    "We'll be using the DRAGEN Thousand Genomes data set, which is available in the [Registry of Open Data on AWS](https://registry.opendata.aws/ilmn-dragen-1kgp/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a variant store, you will need to have uploaded a reference into omics storage, the following helper method will retrieve the reference id for a given reference imported into omics storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_arn(ref_name, client=None):\n",
    "    if not client:\n",
    "        client = boto3.client('omics')\n",
    "    \n",
    "    resp = client.list_reference_stores(maxResults=10)\n",
    "    ref_stores = resp.get('referenceStores')\n",
    "    \n",
    "    # There can only be one reference store per account per region\n",
    "    # if there is a store present, it is the first one\n",
    "    ref_store = ref_stores[0] if ref_stores else None\n",
    "    \n",
    "    if not ref_store:\n",
    "        raise RuntimeError(\"You have not created a reference store, please got to the Amazon Omics Storage tutorial to learn how to create one. Do note continue with this notebook\")\n",
    "        \n",
    "    ref_arn = None\n",
    "    resp = omics.list_references(referenceStoreId=ref_store['id'])\n",
    "    ref_list = resp.get('references')\n",
    "    \n",
    "    for ref in resp.get('references'):\n",
    "        if ref['name'] == ref_name:\n",
    "            ref_arn = ref['arn']\n",
    "    \n",
    "    if ref_arn == None:\n",
    "        raise RuntimeError(f\"Could not find {ref_name}, please got to the Amazon Omics Storage tutorial to learn how to import one. Do note continue with this notebook\")\n",
    "    \n",
    "    return ref_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics.list_reference_stores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Prepare data\n",
    "\n",
    "This tutorial will import a few VCFs from the 1000 Genomes dataset which is available via the [Registry of Open Data on AWS](https://registry.opendata.aws/ilmn-dragen-1kgp/). For demonstration purposes, the bucket and prefix below has only 3 genomes worth of data. Amazon Omics can handle much more, and it is possible to import then entire 1000 Genomes dataset (3202 samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VARIANT_URI = \"s3://1000genomes-dragen/data/precisionFDA/hg38-graph-based/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = urllib.parse.urlparse(SOURCE_VARIANT_URI)\n",
    "bucket = source.netloc\n",
    "prefix = source.path[1:]\n",
    "\n",
    "s3r = boto3.resource('s3')\n",
    "\n",
    "bucket = s3r.Bucket(bucket)\n",
    "objects = bucket.objects.filter(Prefix=prefix, MaxKeys=10_000)\n",
    "ext = 'hard-filtered.vcf.gz'\n",
    "\n",
    "# this is a list comprehension for demo purposes.\n",
    "# if you have a larger set of data a generator comprehension will be more performant here \n",
    "# as it effectively lazy-loads the list of object keys\n",
    "vcf_list = [f\"s3://{o.bucket_name}/{o.key}\" for o in objects if o.key.endswith(ext)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the list of our source VCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Create Variant Store\n",
    "\n",
    "Now, let's create a variant store. Feel free to update `var_store_name`. Note that Variant store names need to match the pattern `[a-z]{1}[a-z0-9_]{254}` (i.e. alphanumeric lowercase with \"_\" up to 255 characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_store_name = f'tutorial_variants_{ts.lower()}'\n",
    "ref_name = 'GRCh38'  ## Change this reference name to match one you have created if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = omics.create_variant_store(\n",
    "    name=var_store_name, \n",
    "    reference={\"referenceArn\": get_reference_arn(ref_name, omics)}\n",
    ")\n",
    "\n",
    "var_store = response\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a variant store takes up to 5 min to complete. We can use a `waiter` to tell us when the store is ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    waiter = omics.get_waiter('variant_store_created')\n",
    "    waiter.wait(name=var_store['name'])\n",
    "\n",
    "    print(f\"variant store {var_store['name']} ready for use\")\n",
    "except botocore.exceptions.WaiterError as e:\n",
    "    print(f\"variant store {var_store['name']} FAILED:\")\n",
    "    print(e)\n",
    "\n",
    "var_store = omics.get_variant_store(name=var_store['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the store is created, navigate to Lake Formation and create resource links as described in the Amazon Omics [documentation](docs.aws.amazon.com/omics). These resource links will be needed for querying the data in Amazon Athena below.\n",
    "\n",
    "In the meantime, you can continue importing data (these can be done in parallel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Import VCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOTAL_JOBS = len(vcf_list)\n",
    "\n",
    "print(f\"importing {N_TOTAL_JOBS} items into {var_store['name']} ...\")\n",
    "for i, uri in enumerate(vcf_list):\n",
    "    \n",
    "    response = omics.start_variant_import_job(\n",
    "        destinationName=var_store['name'],\n",
    "        roleArn=get_role_arn(omics_iam_name),\n",
    "        items=[{\"source\": uri}]\n",
    "    )\n",
    "    \n",
    "    print(f\"[job {i+1} of {N_TOTAL_JOBS}] started import job {response['jobId']} for: {uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant import jobs can take up to 10min to complete, depending on the size of the data being imported. We can monitor the progress of our imports by periodically listing them. This will not block the kernel so we can do other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics.list_variant_import_jobs(filter={\"storeName\": var_store['name']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Query in Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to query your datain Amazon Athena, you need to create resource links to your database using the [AWS LakeFormation Console](https://console.aws.amazon.com/lakeformation/home). You will also need to ensure that the IAM user running this notebook is a Data Lake Administrator. **Note** without both of these in place, the following queries will fail. To satisfy these prerequisites, refer to the instructions provided in the Amazon Omics [documentation](docs.aws.amazon.com/omics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will create resource links to the `default` database in the `AwsDataCatalog` in AWS Glue. It makes a few assumptions to do so - like IAM identity you are using to run this notebook is a Data Lake Administrator and has the permissions to create AWS Glue tables.\n",
    "\n",
    "If you want to be fully sure you are making the correct resource links and providing access to them to the correct identities it is best to create them directly. Refer to the instructions in the Amazon Omics [documentation](docs.aws.amazon.com/omics) on how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to work with AWS RAM and AWS Glue to setup resource links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram = boto3.client('ram')\n",
    "glue = boto3.client('glue')\n",
    "\n",
    "caller_identity = boto3.client('sts').get_caller_identity()\n",
    "AWS_ACCOUNT_ID = caller_identity['Account']\n",
    "AWS_IDENITY_ARN = caller_identity['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll list available shared resources from `OTHER-ACCOUNTS` in AWS RAM and look for the resource that matches the `id` of the Variant store we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ram.list_resources(resourceOwner='OTHER-ACCOUNTS', resourceType='glue:Database')\n",
    "\n",
    "if not response.get('resources'):\n",
    "    print('no shared resources found. verify that you have successfully created an Omics Analytics store')\n",
    "else:\n",
    "    variantstore_resources = [resource for resource in response['resources'] if var_store['id'] in resource['arn']]\n",
    "    if not variantstore_resources:\n",
    "        print(f\"no shared resources matching variant store id {var_store['id']} found\")\n",
    "    else:\n",
    "        variantstore_resource = variantstore_resources[0]\n",
    "\n",
    "variantstore_resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll get the specific resource share the Variant store is associated with. This is so we can get the `owningAccountId` attribute for the share. (Note we could also do this by parsing the `resourceShareArn` for the resource above, but doing it this way is more explicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_share = ram.get_resource_shares(\n",
    "    resourceOwner='OTHER-ACCOUNTS', \n",
    "    resourceShareArns=[variantstore_resource['resourceShareArn']])['resourceShares'][0]\n",
    "resource_share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a table in AWS Glue for the variant store. This is the same as creating a resource link in AWS LakeFormation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a resource link to the table for the variant store and adds it to the `default` database\n",
    "glue.create_table(\n",
    "    DatabaseName='default',\n",
    "    TableInput = {\n",
    "        \"Name\": var_store['name'],\n",
    "        \"TargetTable\": {\n",
    "            \"CatalogId\": resource_share['owningAccountId'],\n",
    "            \"DatabaseName\": f\"variant_{AWS_ACCOUNT_ID}_{var_store['id']}\",\n",
    "            \"Name\": var_store['name'],\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have resource links created, we can start quering the data using [Amazon Athena](https://console.aws.amazon.com/athena). You don't need to wait for all the import jobs to complete to start doing this. Queries can be made while data imports in the background.\n",
    "\n",
    "To query Amazon Omics Analytics stores, you need to use Athena engine version 3. The following code checks if you have an existing Athena workgroup that satisfies this criteria. If not it will create one called `omics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena = boto3.client('athena')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_workgroups = athena.list_work_groups()['WorkGroups']\n",
    "athena_workgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_workgroups = athena.list_work_groups()['WorkGroups']\n",
    "\n",
    "athena_workgroup = None\n",
    "for wg in athena_workgroups:\n",
    "    print(wg['EngineVersion']['EffectiveEngineVersion'])\n",
    "    if wg['EngineVersion']['EffectiveEngineVersion'] == 'Athena engine version 3':\n",
    "        print(f\"Workgroup '{wg['Name']}' found using Athena engine version 3\")\n",
    "        athena_workgroup = wg\n",
    "        break\n",
    "else:\n",
    "    print(\"No workgroups with Athena engine version 3 found. creating one\")\n",
    "    athena_workgroup = athena.create_work_group(\n",
    "        Name='omics',\n",
    "        Configuration={\n",
    "            \"EngineVersion\": {\n",
    "                \"SelectedEngineVersion\": \"Athena engine version 3\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "athena_workgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start writing queries!\n",
    "\n",
    "For fun, let's calculate the TI/TV ratio for these samples. You can navigate to the Athena console or do this from your Jupyter Notebook. This example uses the workgroup `omics` and assumes you have made a resource link to your Variant store in your `default` database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titv_query = f\"\"\"\n",
    "WITH dataset AS (\n",
    "    SELECT \n",
    "        referenceallele,\n",
    "        alternatealleles,\n",
    "        if (cardinality(calls) = 2 and element_at(calls,1)=element_at(calls, 2),2,1) as allelecount, \n",
    "        sampleid\n",
    "    FROM \"default\".\"{var_store['name']}\"\n",
    "    WHERE filters[1] = 'PASS'\n",
    ") \n",
    "SELECT \n",
    "    sampleid, \n",
    "    CAST (sum(CASE WHEN\n",
    "            referenceallele in ('C','T') and alternateallele in ('C','T') THEN allelecount\n",
    "            WHEN referenceallele in ('A','G') and alternateallele in ('A','G') THEN allelecount ELSE 0 END) AS double )/ sum(CASE WHEN\n",
    "            referenceallele in ('C','T') and alternateallele in ('A','G') THEN allelecount\n",
    "            WHEN referenceallele in ('A','G') and alternateallele in ('C','T') THEN allelecount ELSE 0 END) as titv_ratio\n",
    "FROM dataset\n",
    "CROSS JOIN UNNEST(alternatealleles) as t(alternateallele)\n",
    "WHERE \n",
    "    length(referenceallele) = 1 and length(alternateallele) = 1\n",
    "GROUP BY \n",
    "    sampleid\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use AWS Wrangler to submit the query and return results as a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titv = wr.athena.read_sql_query(\n",
    "    titv_query, \n",
    "    database='default', \n",
    "    workgroup=athena_workgroup['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up an Annotation store.\n",
    "\n",
    "Amazon Omics Annotation stores support annotations in VCF, GFF, and TSV formats. In this tutorial, we import [ClinVar](https://www.ncbi.nlm.nih.gov/clinvar/) annotations which can be [downloaded](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/) from the NCBI as a VCF file. Imports need to come from an S3 location, so we'll use a copy of ClinVar used for our Amazon Omics End-to-End blog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_ANNOTATION_URI = \"s3://aws-genomics-datasets/omics-e2e/clinvar.vcf.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating, importing data into, and querying an Annotation store\n",
    "\n",
    "The process of creating, importing data into, and querying an Annotation store is similar to the process you did above for the Variant store, so we'll be brief on the descriptions of each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_store_name = f'tutorial_annotations_{ts.lower()}'\n",
    "ref_name = 'GRCh38'  ## Change this reference name to match one you have created if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = omics.create_annotation_store(\n",
    "    name=ann_store_name, \n",
    "    reference={\"referenceArn\": get_reference_arn(ref_name, omics)},\n",
    "    storeFormat='VCF'  ## <<--- THIS IS UNIQUE TO ANNOTATION STORES\n",
    ")\n",
    "\n",
    "ann_store = response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    waiter = omics.get_waiter('annotation_store_created')\n",
    "    waiter.wait(name=ann_store['name'])\n",
    "\n",
    "    print(f\"annotation store {ann_store['name']} ready for use\")\n",
    "except botocore.exceptions.WaiterError as e:\n",
    "    print(f\"annotation store {ann_store['name']} FAILED:\")\n",
    "    print(e)\n",
    "\n",
    "ann_store = omics.get_annotation_store(name=ann_store['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing data ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ClinVar dataset relatively small, this should take under 2min to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = omics.start_annotation_import_job(\n",
    "    destinationName=ann_store['name'],\n",
    "    roleArn=get_role_arn(omics_iam_name),\n",
    "    items=[{\"source\": SOURCE_ANNOTATION_URI}]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics.list_annotation_import_jobs(filter={\"storeName\": ann_store['name']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a resource link to the Annotation store is the same as with the Variant store. We'll do this all in one cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ram.list_resources(resourceOwner='OTHER-ACCOUNTS', resourceType='glue:Database')\n",
    "\n",
    "if not response.get('resources'):\n",
    "    print('no shared resources found. verify that you have successfully created an Omics Analytics store')\n",
    "else:\n",
    "    annotationstore_resources = [resource for resource in response['resources'] if ann_store['id'] in resource['arn']]\n",
    "    if not annotationstore_resources:\n",
    "        print(f\"no shared resources matching annotation store id {ann_store['id']} found\")\n",
    "    else:\n",
    "        annotationstore_resource = annotationstore_resources[0]\n",
    "\n",
    "        resource_share = ram.get_resource_shares(\n",
    "            resourceOwner='OTHER-ACCOUNTS', \n",
    "            resourceShareArns=[annotationstore_resource['resourceShareArn']])['resourceShares'][0]\n",
    "        \n",
    "        # this creates a resource link to the table for the annotation store and adds it to the `default` database\n",
    "        glue.create_table(\n",
    "            DatabaseName='default',\n",
    "            TableInput = {\n",
    "                \"Name\": ann_store['name'],\n",
    "                \"TargetTable\": {\n",
    "                    \"CatalogId\": resource_share['owningAccountId'],\n",
    "                    \"DatabaseName\": f\"annotation_{AWS_ACCOUNT_ID}_{ann_store['id']}\",\n",
    "                    \"Name\": ann_store['name'],\n",
    "                }\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the data and returning a Pandas Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this query identifies single nucleotide variants associated \n",
    "# with Non-small cell lung cancer (SNOMED_CT 254637007)\n",
    "# with \"drug_response\" clinical significance\n",
    "ncsclc_query = f\"\"\"\n",
    "SELECT \n",
    "    concat('chr', contigname) as contigname, \n",
    "    start, \n",
    "    referenceallele,\n",
    "    alternatealleles,\n",
    "    attributes['GENEINFO'] as gene_info,\n",
    "    attributes['CLNSIG'] as clinical_significance, \n",
    "    regexp_extract_all(attributes['CLNDISDB'], 'SNOMED_CT:\\d+') as snomed_ct, \n",
    "    attributes['CLNDN'] as disease_name\n",
    "from \"default\".\"{ann_store['name']}\"\n",
    "where \n",
    "        attributes['CLNDISDB'] like '%SNOMED_CT:254637007%'\n",
    "    and length(referenceallele) = 1\n",
    "    and cardinality(alternatealleles) = 1\n",
    "    and length(alternatealleles[1]) = 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsclc = wr.athena.read_sql_query(\n",
    "    ncsclc_query, \n",
    "    database='default', \n",
    "    workgroup=athena_workgroup['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsclc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
